{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650953982510
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy # !{sys.executable} ensures package installation in conda env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650953984589
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650953984820
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load and transform data\n",
    "from os import path, mkdir\n",
    "if not path.isdir(\"data/\"):\n",
    "    mkdir(\"data/\")\n",
    "if not path.isdir(\"models/\"):\n",
    "    mkdir(\"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!curl https://groups.csail.mit.edu/sls/downloads/movie/engtest.bio -o data/test.txt\n",
    "!curl https://groups.csail.mit.edu/sls/downloads/movie/engtrain.bio -o data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650953985923
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def load_data_spacy(file_path):\n",
    "    ''' Converts data from:\n",
    "    label \\t word \\n label \\t word \\n \\n label \\t word\n",
    "    to: sentence, {entities : [(start, end, label), (stard, end, label)]}\n",
    "    '''\n",
    "    file = open(file_path, 'r')\n",
    "    training_data, entities, sentence, unique_labels = [], [], [], []\n",
    "    current_annotation = None\n",
    "    end = 0 # initialize counter to keep track of start and end characters\n",
    "    for line in file:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        # lines with len > 1 are words\n",
    "        if len(line) > 1:\n",
    "            label = line[0][2:]     # the .txt is formatted: label \\t word, label[0:2] = label_type\n",
    "            label_type = line[0][0] # beginning of annotations - \"B\", intermediate - \"I\"\n",
    "            word = line[1]\n",
    "            sentence.append(word)\n",
    "            end += (len(word) + 1)  # length of the word + trailing space\n",
    "            \n",
    "            if label_type != 'I' and current_annotation:  # if at the end of an annotation\n",
    "                entities.append((start, end - 2 - len(word), current_annotation))  # append the annotation\n",
    "                current_annotation = None                 # reset the annotation\n",
    "            if label_type == 'B':                         # if beginning new annotation\n",
    "                start = end - len(word) - 1  # start annotation at beginning of word\n",
    "                current_annotation = label   # append the word to the current annotation\n",
    "            if label_type == 'I':            # if the annotation is multi-word\n",
    "                current_annotation = label   # append the word\n",
    "            \n",
    "            if label != 'O' and label not in unique_labels:\n",
    "                unique_labels.append(label)\n",
    " \n",
    "        # lines with len == 1 are breaks between sentences\n",
    "        if len(line) == 1: \n",
    "            if current_annotation:\n",
    "                entities.append((start, end - 1, current_annotation))\n",
    "            sentence = \" \".join(sentence)\n",
    "            training_data.append([sentence, {'entities' : entities}])\n",
    "            # reset the counters and temporary lists\n",
    "            end = 0            \n",
    "            entities, sentence = [], []\n",
    "            current_annotation = None\n",
    "    file.close()\n",
    "    return training_data, unique_labels            \n",
    "            \n",
    "TRAIN_DATA, LABELS = load_data_spacy(\"data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650953986244
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "[x[0] for x in TRAIN_DATA[1:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650953986532
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "[x[1] for x in TRAIN_DATA[1:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650953992554
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650953994748
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "TEST_DATA, _ = load_data_spacy(\"data/test.txt\")\n",
    "\n",
    "test_sentences = [x[0] for x in TEST_DATA[0:15]] # extract the sentences from [sentence, entity]\n",
    "for x in test_sentences:\n",
    "    doc = nlp(x)\n",
    "    displacy.render(doc, jupyter = True, style = \"ent\")\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650954305478
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# A simple decorator to log function processing time\n",
    "def timer(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        print(\"Completed in {} seconds\".format(int(te - ts)))\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "# Data must be of the form (sentence, {entities: [start, end, label]})\n",
    "@timer\n",
    "def train_spacy(train_data, labels, iterations, dropout = 0.2, display_freq = 1):\n",
    "    ''' Train a spacy NER model, which can be queried against with test data\n",
    "    \n",
    "    train_data : training data in the format of (sentence, {entities: [(start, end, label)]})\n",
    "    labels : a list of unique annotations\n",
    "    iterations : number of training iterations\n",
    "    dropout : dropout proportion for training\n",
    "    display_freq : number of epochs between logging losses to console\n",
    "    '''\n",
    "    nlp = spacy.blank('en') \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe('ner')\n",
    "    \n",
    "    # Add entity labels to the NER pipeline\n",
    "    for i in labels:\n",
    "        ner.add_label(i)\n",
    "\n",
    "    # Disable other pipelines in SpaCy to only train NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        nlp.vocab.vectors.name = 'spacy_model' # without this, spaCy throws an \"unnamed\" error\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itr in range(iterations):\n",
    "            random.shuffle(train_data) # shuffle the training data before each iteration\n",
    "            losses = {}\n",
    "            batches = minibatch(train_data, size = compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(           \n",
    "                    texts,\n",
    "                    annotations, \n",
    "                    drop = dropout,   \n",
    "                    sgd = optimizer,\n",
    "                    losses = losses)\n",
    "            if itr % display_freq == 0:\n",
    "                print(\"Iteration {} Loss: {}\".format(itr + 1, losses))\n",
    "    return nlp\n",
    "\n",
    "# Train (and save) the NER model\n",
    "ner = train_spacy(TRAIN_DATA, LABELS,6)\n",
    "ner.to_disk(\"models/spacy_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650954307209
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "def load_model(model_path):\n",
    "    ''' Loads a pre-trained model for prediction on new test sentences\n",
    "    \n",
    "    model_path : directory of model saved by spacy.to_disk\n",
    "    '''\n",
    "    nlp = spacy.blank('en') \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    ner = nlp.from_disk(model_path)\n",
    "    return ner\n",
    "\n",
    "ner = load_model(\"models/spacy_example\")\n",
    "\n",
    "TEST_DATA, _ = load_data_spacy(\"data/test.txt\")\n",
    "\n",
    "test_sentences = [x[0] for x in TEST_DATA[0:15]] # extract the sentences from [sentence, entity]\n",
    "for x in test_sentences:\n",
    "    doc = ner(x)\n",
    "    displacy.render(doc, jupyter = True, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
